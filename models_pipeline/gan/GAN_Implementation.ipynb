{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serbian Dish Image Generator - GAN Implementation\n",
    "\n",
    "This notebook implements a conditional GAN system for generating Serbian food images from text prompts using CLIP embeddings. It combines the functionality of the individual scripts (`dataset.py`, `models.py`, `train_with_plotting.py`, etc.) into a single interactive environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import math\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms as T\n",
    "from torchvision.utils import save_image, make_grid\n",
    "\n",
    "# Determine device\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Loading (`dataset.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_EXTS = ('.jpg', '.jpeg', '.png', '.webp')\n",
    "\n",
    "class CaptionImageSet(Dataset):\n",
    "    def __init__(self, root=\"data/processed\", size=128, embeddings_dir=\"embedds\"):\n",
    "        img_dir = os.path.join(root, \"images\")\n",
    "        # Allow custom embeddings directory name\n",
    "        emb_dir = os.path.join(root, embeddings_dir)\n",
    "\n",
    "        ids = []\n",
    "        # Check if directories exist before globbing\n",
    "        if os.path.exists(img_dir) and os.path.exists(emb_dir):\n",
    "            for p in glob.glob(os.path.join(img_dir, \"*\")):\n",
    "                base, ext = os.path.splitext(os.path.basename(p))\n",
    "                if ext.lower() in IMG_EXTS and os.path.exists(os.path.join(emb_dir, base + \".npy\")):\n",
    "                    ids.append(base)\n",
    "            ids.sort()\n",
    "        else:\n",
    "            print(f\"Warning: directories {img_dir} or {emb_dir} not found.\")\n",
    "\n",
    "        if not ids and os.path.exists(img_dir):\n",
    "            print(f\"Warning: No image/embedding pairs found in {root}\")\n",
    "            # raise RuntimeError(f\"No image/embedding pairs found. Check {img_dir} and {emb_dir}.\")\n",
    "\n",
    "        self.ids = ids\n",
    "        self.img_dir, self.emb_dir = img_dir, emb_dir\n",
    "\n",
    "        if ids:\n",
    "            first_emb_path = os.path.join(emb_dir, ids[0] + \".npy\")\n",
    "            try:\n",
    "                first_emb = np.load(first_emb_path)\n",
    "                print(f\"Dataset info: {len(ids)} samples, embedding dim: {first_emb.shape}\")\n",
    "            except:\n",
    "                print(f\"Dataset info: {len(ids)} samples\")\n",
    "\n",
    "        self.tf = T.Compose([\n",
    "            T.Resize(size, interpolation=T.InterpolationMode.BICUBIC),\n",
    "            T.CenterCrop(size),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize([0.5]*3, [0.5]*3)  # [-1,1]\n",
    "        ])\n",
    "\n",
    "    def __len__(self): return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        id_ = self.ids[i]\n",
    "\n",
    "        # Improved image loading with proper error handling\n",
    "        try:\n",
    "            img_path = os.path.join(self.img_dir, id_ + \".jpg\")\n",
    "            if os.path.exists(img_path):\n",
    "                img = Image.open(img_path).convert(\"RGB\")\n",
    "            else:\n",
    "                # Look for other supported formats\n",
    "                alt_paths = [os.path.join(self.img_dir, id_ + ext) for ext in IMG_EXTS]\n",
    "                found_path = next((p for p in alt_paths if os.path.exists(p)), None)\n",
    "\n",
    "                if found_path is None:\n",
    "                    raise FileNotFoundError(f\"No image found for ID '{id_}' in {self.img_dir}\")\n",
    "\n",
    "                img = Image.open(found_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to load image for ID '{id_}': {str(e)}\")\n",
    "\n",
    "        # Improved embedding loading with error handling\n",
    "        try:\n",
    "            emb_path = os.path.join(self.emb_dir, id_ + \".npy\")\n",
    "            if not os.path.exists(emb_path):\n",
    "                raise FileNotFoundError(f\"No embedding found for ID '{id_}' at {emb_path}\")\n",
    "\n",
    "            e = np.load(emb_path).astype(\"float32\")\n",
    "\n",
    "            if e.size == 0:\n",
    "                raise ValueError(f\"Empty embedding file for ID '{id_}'\")\n",
    "\n",
    "            # Normalize embedding\n",
    "            norm = np.linalg.norm(e)\n",
    "            if norm < 1e-8:\n",
    "                raise ValueError(f\"Zero or near-zero embedding norm for ID '{id_}'\")\n",
    "\n",
    "            e /= norm\n",
    "            e = torch.from_numpy(e)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to load embedding for ID '{id_}': {str(e)}\")\n",
    "\n",
    "        # Apply image transform\n",
    "        try:\n",
    "            x = self.tf(img)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to transform image for ID '{id_}': {str(e)}\")\n",
    "\n",
    "        return x, e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Differentiable Augmentation (`diffaug.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUG_POLICIES = [\"color\", \"translation\", \"cutout\"]\n",
    "\n",
    "def rand_brightness(x): \n",
    "    return x + (torch.rand(x.size(0), 1, 1, 1, device=x.device) - 0.5)\n",
    "\n",
    "def rand_saturation(x):\n",
    "    x_mean = x.mean(dim=1, keepdim=True)\n",
    "    return (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, device=x.device) * 2) + x_mean\n",
    "\n",
    "def rand_contrast(x):\n",
    "    x_mean = x.mean(dim=(1,2,3), keepdim=True)\n",
    "    return (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, device=x.device) + 0.5) + x_mean\n",
    "\n",
    "def color(x):\n",
    "    for fn in (rand_brightness, rand_saturation, rand_contrast):\n",
    "        x = fn(x)\n",
    "    return x\n",
    "\n",
    "def translation(x, ratio=0.125):\n",
    "    B, C, H, W = x.shape\n",
    "    shift_x = int(H * ratio + 0.5)\n",
    "    shift_y = int(W * ratio + 0.5)\n",
    "    \n",
    "    translation_x = torch.randint(-shift_x, shift_x + 1, size=(B,), device=x.device)\n",
    "    translation_y = torch.randint(-shift_y, shift_y + 1, size=(B,), device=x.device)\n",
    "    \n",
    "    grid_x = torch.arange(W, dtype=torch.float32, device=x.device).view(1, 1, W).repeat(B, H, 1)\n",
    "    grid_y = torch.arange(H, dtype=torch.float32, device=x.device).view(1, H, 1).repeat(B, 1, W)\n",
    "    \n",
    "    grid_x = grid_x + translation_x.view(B, 1, 1)\n",
    "    grid_y = grid_y + translation_y.view(B, 1, 1)\n",
    "    \n",
    "    grid_x = 2.0 * grid_x / (W - 1) - 1.0\n",
    "    grid_y = 2.0 * grid_y / (H - 1) - 1.0\n",
    "    \n",
    "    grid = torch.stack([grid_x, grid_y], dim=-1)\n",
    "    x_translated = F.grid_sample(x, grid, mode='bilinear', padding_mode='border', align_corners=True)\n",
    "    \n",
    "    return x_translated\n",
    "\n",
    "def cutout(x, ratio=0.5):\n",
    "    B, C, H, W = x.shape\n",
    "    cut_h, cut_w = int(H * ratio + 0.5), int(W * ratio + 0.5)\n",
    "    mask = torch.ones_like(x)\n",
    "    \n",
    "    for i in range(B):\n",
    "        cy = torch.randint(0, H, (1,), device=x.device).item()\n",
    "        cx = torch.randint(0, W, (1,), device=x.device).item()\n",
    "        y1 = max(0, cy - cut_h // 2)\n",
    "        y2 = min(H, y1 + cut_h)\n",
    "        x1 = max(0, cx - cut_w // 2)\n",
    "        x2 = min(W, x1 + cut_w)\n",
    "        mask[i, :, y1:y2, x1:x2] = 0\n",
    "    \n",
    "    return x * mask\n",
    "\n",
    "def diff_augment(x, policies=AUG_POLICIES):\n",
    "    for p in policies:\n",
    "        if p == \"color\": x = color(x)\n",
    "        elif p == \"translation\": x = translation(x)\n",
    "        elif p == \"cutout\": x = cutout(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exponential Moving Average (`ema.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMA:\n",
    "    def __init__(self, model, decay=0.999):\n",
    "        self.decay = decay\n",
    "        self.shadow = {}\n",
    "        self.backup = {}\n",
    "        for name, p in model.named_parameters():\n",
    "            if p.requires_grad:\n",
    "                self.shadow[name] = p.data.clone()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update(self, model):\n",
    "        for name, p in model.named_parameters():\n",
    "            if p.requires_grad:\n",
    "                assert name in self.shadow\n",
    "                new_avg = self.shadow[name] * self.decay + p.data * (1.0 - self.decay)\n",
    "                self.shadow[name] = new_avg.clone()\n",
    "\n",
    "    def apply_shadow(self, model):\n",
    "        self.backup = {}\n",
    "        for name, p in model.named_parameters():\n",
    "            if p.requires_grad:\n",
    "                self.backup[name] = p.data.clone()\n",
    "                p.data = self.shadow[name].clone()\n",
    "\n",
    "    def copy_to(self, model):\n",
    "        \"\"\"Copy shadow weights to model without creating backup.\"\"\"\n",
    "        for name, p in model.named_parameters():\n",
    "            if p.requires_grad:\n",
    "                p.data = self.shadow[name].clone()\n",
    "\n",
    "    def restore(self, model):\n",
    "        for name, p in model.named_parameters():\n",
    "            if p.requires_grad:\n",
    "                p.data = self.backup[name].clone()\n",
    "        self.backup = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Architectures (`models.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CondBN(nn.Module):\n",
    "    def __init__(self, ch, cond_dim):\n",
    "        super().__init__()\n",
    "        self.bn = nn.BatchNorm2d(ch, affine=False)\n",
    "        self.gam = nn.Linear(cond_dim, ch)\n",
    "        self.bet = nn.Linear(cond_dim, ch)\n",
    "    def forward(self, x, y):\n",
    "        h = self.bn(x)\n",
    "        g = self.gam(y).unsqueeze(-1).unsqueeze(-1)\n",
    "        b = self.bet(y).unsqueeze(-1).unsqueeze(-1)\n",
    "        return h * (1 + g) + b\n",
    "\n",
    "class ResBlockG(nn.Module):\n",
    "    def __init__(self, in_c, out_c, cond_dim, up=True):\n",
    "        super().__init__()\n",
    "        self.up = up\n",
    "        self.cbn1 = CondBN(in_c, cond_dim)\n",
    "        self.cbn2 = CondBN(out_c, cond_dim)\n",
    "        self.conv1 = nn.Conv2d(in_c, out_c, 3, 1, 1)\n",
    "        self.conv2 = nn.Conv2d(out_c, out_c, 3, 1, 1)\n",
    "        self.skip = nn.Conv2d(in_c, out_c, 1) if in_c!=out_c else nn.Identity()\n",
    "    def forward(self, x, y):\n",
    "        h = self.cbn1(x, y); h = F.relu(h)\n",
    "        if self.up: h = F.interpolate(h, scale_factor=2, mode=\"nearest\")\n",
    "        h = self.conv1(h)\n",
    "        h = self.cbn2(h, y); h = F.relu(h); h = self.conv2(h)\n",
    "        s = x\n",
    "        if self.up: s = F.interpolate(s, scale_factor=2, mode=\"nearest\")\n",
    "        s = self.skip(s)\n",
    "        return h + s\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim=128, cond_in=768, cond_hidden=256, base_ch=64, out_size=128):\n",
    "        super().__init__()\n",
    "        self.cond = nn.Sequential(\n",
    "            nn.Linear(cond_in, 512), nn.ReLU(True),\n",
    "            nn.Linear(512, cond_hidden)\n",
    "        )\n",
    "        self.fc = nn.Linear(z_dim + cond_hidden, 4*4*base_ch*16)\n",
    "        ch = base_ch*16\n",
    "        blocks, size = [], 4\n",
    "        while size < out_size:\n",
    "            blocks.append(ResBlockG(ch, ch//2, cond_dim=cond_hidden, up=True))\n",
    "            ch //= 2; size *= 2\n",
    "        self.blocks = nn.ModuleList(blocks)\n",
    "        self.bn = nn.BatchNorm2d(ch, affine=True)\n",
    "        self.conv_out = nn.Conv2d(ch, 3, 3, 1, 1)\n",
    "\n",
    "    def forward(self, z, e):\n",
    "        y = self.cond(e)\n",
    "        h = self.fc(torch.cat([z, y], dim=1))\n",
    "        ch = h.shape[1] // (4*4)\n",
    "        h = h.view(-1, ch, 4, 4)\n",
    "        for b in self.blocks: h = b(h, y)\n",
    "        h = F.relu(self.bn(h))\n",
    "        x = torch.tanh(self.conv_out(h))\n",
    "        return x\n",
    "\n",
    "class ResBlockD(nn.Module):\n",
    "    def __init__(self, in_c, out_c, down=True):\n",
    "        super().__init__()\n",
    "        self.down = down\n",
    "        self.conv1 = nn.Conv2d(in_c, out_c, 3, 1, 1)\n",
    "        self.conv2 = nn.Conv2d(out_c, out_c, 3, 1, 1)\n",
    "        self.skip = nn.Conv2d(in_c, out_c, 1) if in_c!=out_c else nn.Identity()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = F.relu(x); h = self.conv1(h)\n",
    "        h = F.relu(h); h = self.conv2(h)\n",
    "        s = x\n",
    "        if self.down:\n",
    "            h = F.avg_pool2d(h, 2)\n",
    "            s = F.avg_pool2d(s, 2)\n",
    "        s = self.skip(s)\n",
    "        return h + s\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, cond_in=768, base_ch=64, in_size=128):\n",
    "        super().__init__()\n",
    "        ch = base_ch\n",
    "        blocks = [ResBlockD(3, ch, down=True)]\n",
    "        size, c = in_size, ch\n",
    "        while size > 4:\n",
    "            blocks.append(ResBlockD(c, c*2, down=True))\n",
    "            c *= 2; size //= 2\n",
    "        self.blocks = nn.ModuleList(blocks)\n",
    "        self.conv_out = nn.Conv2d(c, c, 3, 1, 1)\n",
    "        self.lin = nn.Linear(c, 1)\n",
    "        self.embed = nn.Linear(cond_in, c)\n",
    "\n",
    "    def forward(self, x, e):\n",
    "        h = x\n",
    "        for b in self.blocks: h = b(h)\n",
    "        h = F.relu(self.conv_out(h))\n",
    "        h = torch.sum(h, dim=(2,3))\n",
    "        out = self.lin(h) + torch.sum(self.embed(e) * h, dim=1, keepdim=True)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Logic (`train_with_plotting.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hinge_d(real_logits, fake_logits, mis_logits=None, mis_weight=0.5):\n",
    "    loss = F.relu(1 - real_logits).mean() + F.relu(1 + fake_logits).mean()\n",
    "    if mis_logits is not None:\n",
    "        loss = loss + mis_weight * F.relu(1 + mis_logits).mean()\n",
    "    return loss\n",
    "\n",
    "def hinge_g(fake_logits):\n",
    "    return -fake_logits.mean()\n",
    "\n",
    "def r1_penalty(real_x, real_logits):\n",
    "    grad = torch.autograd.grad(\n",
    "        outputs=real_logits.sum(), inputs=real_x,\n",
    "        create_graph=True, retain_graph=True, only_inputs=True\n",
    "    )[0]\n",
    "    penalty = grad.pow(2).reshape(grad.size(0), -1).sum(dim=1).mean()\n",
    "    return penalty\n",
    "\n",
    "def save_samples(path, imgs):\n",
    "    grid = make_grid((imgs.clamp(-1,1)+1)/2, nrow=int(math.sqrt(imgs.size(0))+0.5))\n",
    "    save_image(grid, path)\n",
    "\n",
    "def plot_losses(log_data, output_dir):\n",
    "    \"\"\"Plot and save training curves.\"\"\"\n",
    "    if not log_data: return\n",
    "    \n",
    "    steps = [x['step'] for x in log_data]\n",
    "    d_losses = [x['d_loss'] for x in log_data]\n",
    "    g_losses = [x['g_loss'] for x in log_data]\n",
    "    real_logits = [x.get('real_logits_mean', 0) for x in log_data]\n",
    "    fake_logits = [x.get('fake_logits_mean', 0) for x in log_data]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Loss curves\n",
    "    axes[0, 0].plot(steps, d_losses, label='D Loss', alpha=0.7)\n",
    "    axes[0, 0].plot(steps, g_losses, label='G Loss', alpha=0.7)\n",
    "    axes[0, 0].set_xlabel('Steps')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].set_title('Training Losses')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # D loss only (zoomed)\n",
    "    axes[0, 1].plot(steps, d_losses, color='blue', alpha=0.7)\n",
    "    axes[0, 1].set_xlabel('Steps')\n",
    "    axes[0, 1].set_ylabel('D Loss')\n",
    "    axes[0, 1].set_title('Discriminator Loss')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # G loss only (zoomed)\n",
    "    axes[1, 0].plot(steps, g_losses, color='orange', alpha=0.7)\n",
    "    axes[1, 0].set_xlabel('Steps')\n",
    "    axes[1, 0].set_ylabel('G Loss')\n",
    "    axes[1, 0].set_title('Generator Loss')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Logits (real vs fake scores)\n",
    "    axes[1, 1].plot(steps, real_logits, label='Real Logits', alpha=0.7)\n",
    "    axes[1, 1].plot(steps, fake_logits, label='Fake Logits', alpha=0.7)\n",
    "    axes[1, 1].set_xlabel('Steps')\n",
    "    axes[1, 1].set_ylabel('Logits')\n",
    "    axes[1, 1].set_title('Discriminator Outputs')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    axes[1, 1].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'training_curves.png'), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # ----- Data\n",
    "    ds = CaptionImageSet(args.data_root, size=args.img_size, embeddings_dir=args.embeddings_dir)\n",
    "    if len(ds) == 0:\n",
    "        print(\"No data found. Exiting training.\")\n",
    "        return\n",
    "        \n",
    "    emb_dim = ds[0][1].numel()\n",
    "    dl = DataLoader(ds, batch_size=args.batch, shuffle=True,\n",
    "                    num_workers=args.num_workers, drop_last=True, pin_memory=True)\n",
    "\n",
    "    # ----- Models\n",
    "    G = Generator(z_dim=args.z_dim, cond_in=emb_dim, cond_hidden=args.cond_dim,\n",
    "                  base_ch=args.base_ch, out_size=args.img_size).to(device)\n",
    "    D = Discriminator(cond_in=emb_dim, base_ch=args.base_ch, in_size=args.img_size).to(device)\n",
    "\n",
    "    optG = torch.optim.Adam(G.parameters(), lr=args.g_lr, betas=(0.0, 0.9))\n",
    "    optD = torch.optim.Adam(D.parameters(), lr=args.d_lr, betas=(0.0, 0.9))\n",
    "\n",
    "    ema = EMA(G, decay=args.ema)\n",
    "\n",
    "    # Use AMP if on CUDA\n",
    "    scaler = torch.amp.GradScaler('cuda', enabled=(device==\"cuda\"))\n",
    "\n",
    "    # Resume from checkpoint if specified\n",
    "    start_step = 0\n",
    "    if args.resume_from and os.path.exists(args.resume_from):\n",
    "        print(f\"Loading checkpoint from {args.resume_from}\")\n",
    "        checkpoint = torch.load(args.resume_from, map_location=device)\n",
    "        G.load_state_dict(checkpoint[\"G\"])\n",
    "        D.load_state_dict(checkpoint[\"D\"])\n",
    "        optG.load_state_dict(checkpoint[\"optG\"])\n",
    "        optD.load_state_dict(checkpoint[\"optD\"])\n",
    "        ema.shadow = checkpoint[\"ema\"]\n",
    "        start_step = checkpoint[\"step\"]\n",
    "        print(f\"Resumed from step {start_step}\")\n",
    "\n",
    "    os.makedirs(args.out_dir, exist_ok=True)\n",
    "    \n",
    "    # Fixed samples for visualization\n",
    "    fixed = next(iter(dl))\n",
    "    actual_n_sample = min(args.n_sample, args.batch)\n",
    "    fixed_e = fixed[1][:actual_n_sample].to(device)\n",
    "    fixed_z = torch.randn(actual_n_sample, args.z_dim, device=device)\n",
    "    print(f\"Using {actual_n_sample} samples for visualization\")\n",
    "\n",
    "    # Logging\n",
    "    log_data = []\n",
    "    log_file = os.path.join(args.out_dir, 'training_log.json')\n",
    "\n",
    "    # Load existing log data if resuming\n",
    "    if args.resume_from and os.path.exists(log_file):\n",
    "        try:\n",
    "            with open(log_file, 'r') as f:\n",
    "                log_data = json.load(f)\n",
    "            print(f\"Loaded {len(log_data)} existing log entries\")\n",
    "        except:\n",
    "            print(\"Could not load existing log data, starting fresh\")\n",
    "            log_data = []\n",
    "\n",
    "    step = start_step\n",
    "    pbar = tqdm(total=args.iters, desc=\"training\", initial=start_step)\n",
    "\n",
    "    while step < args.iters:\n",
    "        for x, e in dl:\n",
    "            x, e = x.to(device, non_blocking=True), e.to(device, non_blocking=True)\n",
    "            B = x.size(0)\n",
    "\n",
    "            # Create mismatched text by random permutation\n",
    "            perm_idx = torch.randperm(B)\n",
    "            while torch.equal(perm_idx, torch.arange(B)) and B > 1:\n",
    "                perm_idx = torch.randperm(B)\n",
    "            e_mis = e[perm_idx]\n",
    "\n",
    "            # ----------------- D update -----------------\n",
    "            for _ in range(args.n_disc):\n",
    "                z = torch.randn(B, args.z_dim, device=device)\n",
    "                with torch.no_grad():\n",
    "                    x_fake = G(z, e).detach()\n",
    "\n",
    "                xr, xf = x, x_fake\n",
    "                if args.diffaugment:\n",
    "                    xr = diff_augment(xr)\n",
    "                    xf = diff_augment(xf)\n",
    "\n",
    "                xr.requires_grad_(True)\n",
    "                # AMP context\n",
    "                with torch.amp.autocast('cuda', enabled=(device==\"cuda\")):\n",
    "                    real_logits = D(xr, e)\n",
    "                    fake_logits = D(xf, e)\n",
    "                    mis_logits  = D(xr, e_mis) if args.use_mismatch else None\n",
    "                    d_loss = hinge_d(real_logits, fake_logits, mis_logits, mis_weight=args.mismatch_w)\n",
    "\n",
    "                # Compute total discriminator loss (including R1 if needed)\n",
    "                total_d_loss = d_loss\n",
    "                if (step % args.r1_every) == 0:\n",
    "                    with torch.amp.autocast('cuda', enabled=False):\n",
    "                        r1 = r1_penalty(xr, real_logits)\n",
    "                    total_d_loss = d_loss + (args.r1_gamma/2) * r1\n",
    "\n",
    "                optD.zero_grad(set_to_none=True)\n",
    "                if device==\"cuda\":\n",
    "                    scaler.scale(total_d_loss).backward()\n",
    "                else:\n",
    "                    total_d_loss.backward()\n",
    "\n",
    "                if device==\"cuda\":\n",
    "                    if args.grad_clip > 0:\n",
    "                        scaler.unscale_(optD)\n",
    "                        torch.nn.utils.clip_grad_norm_(D.parameters(), args.grad_clip)\n",
    "                    scaler.step(optD)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    if args.grad_clip > 0:\n",
    "                        torch.nn.utils.clip_grad_norm_(D.parameters(), args.grad_clip)\n",
    "                    optD.step()\n",
    "\n",
    "            # ----------------- G update -----------------\n",
    "            z = torch.randn(B, args.z_dim, device=device)\n",
    "            with torch.amp.autocast('cuda', enabled=(device==\"cuda\")):\n",
    "                x_fake = G(z, e)\n",
    "                xf = diff_augment(x_fake) if args.diffaugment else x_fake\n",
    "                g_loss = hinge_g(D(xf, e))\n",
    "\n",
    "            optG.zero_grad(set_to_none=True)\n",
    "            if device==\"cuda\":\n",
    "                scaler.scale(g_loss).backward()\n",
    "                if args.grad_clip > 0:\n",
    "                    scaler.unscale_(optG)\n",
    "                    torch.nn.utils.clip_grad_norm_(G.parameters(), args.grad_clip)\n",
    "                scaler.step(optG); scaler.update()\n",
    "            else:\n",
    "                g_loss.backward()\n",
    "                if args.grad_clip > 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(G.parameters(), args.grad_clip)\n",
    "                optG.step()\n",
    "\n",
    "            # EMA update\n",
    "            ema.update(G)\n",
    "\n",
    "            # ----------------- Logging -----------------\n",
    "            if step % args.log_every == 0:\n",
    "                log_entry = {\n",
    "                    'step': step,\n",
    "                    'd_loss': float(d_loss.detach().cpu()),\n",
    "                    'g_loss': float(g_loss.detach().cpu()),\n",
    "                    'real_logits_mean': float(real_logits.mean().detach().cpu()),\n",
    "                    'fake_logits_mean': float(fake_logits.mean().detach().cpu()),\n",
    "                }\n",
    "                log_data.append(log_entry)\n",
    "                \n",
    "                # Save log file\n",
    "                with open(log_file, 'w') as f:\n",
    "                    json.dump(log_data, f, indent=2)\n",
    "                \n",
    "                # Plot curves\n",
    "                if step > 0 and step % args.plot_every == 0:\n",
    "                    plot_losses(log_data, args.out_dir)\n",
    "\n",
    "            # ----------------- Samples -----------------\n",
    "            if (step % args.sample_every) == 0:\n",
    "                ema.apply_shadow(G)\n",
    "                with torch.no_grad():\n",
    "                    imgs = G(fixed_z, fixed_e)\n",
    "                    save_samples(os.path.join(args.out_dir, f\"samples_{step:07d}.png\"), imgs)\n",
    "                ema.restore(G)\n",
    "\n",
    "            # ----------------- Checkpoints -----------------\n",
    "            if (step % args.ckpt_every) == 0 and step > 0:\n",
    "                torch.save({\n",
    "                    \"G\": G.state_dict(), \"D\": D.state_dict(),\n",
    "                    \"optG\": optG.state_dict(), \"optD\": optD.state_dict(),\n",
    "                    \"ema\": ema.shadow, \"step\": step,\n",
    "                    \"args\": vars(args)\n",
    "                }, os.path.join(args.out_dir, f\"ckpt_{step:07d}.pt\"))\n",
    "\n",
    "            step += 1\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix({\n",
    "                \"d_loss\": f\"{float(d_loss.detach().cpu()):.4f}\", \n",
    "                \"g_loss\": f\"{float(g_loss.detach().cpu()):.4f}\"\n",
    "            })\n",
    "\n",
    "            if step >= args.iters:\n",
    "                break\n",
    "\n",
    "    # Final plot\n",
    "    plot_losses(log_data, args.out_dir)\n",
    "    pbar.close()\n",
    "    print(f\"Training completed! Check {args.out_dir} for results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    data_root = \"data/processed\"\n",
    "    embeddings_dir = \"enhanced_768d_embeds\"\n",
    "    out_dir = \"runs/cgan_notebook\"\n",
    "    img_size = 128\n",
    "    z_dim = 128\n",
    "    cond_dim = 512\n",
    "    base_ch = 32\n",
    "    batch = 4\n",
    "    iters = 2000 \n",
    "    n_disc = 1\n",
    "    g_lr = 5e-5\n",
    "    d_lr = 1.5e-4\n",
    "    grad_clip = 1.0\n",
    "    num_workers = 0 \n",
    "    diffaugment = True\n",
    "    use_mismatch = True\n",
    "    mismatch_w = 0.5\n",
    "    r1_every = 16\n",
    "    r1_gamma = 2.0\n",
    "    ema = 0.999\n",
    "    sample_every = 200\n",
    "    ckpt_every = 500\n",
    "    n_sample = 8\n",
    "    log_every = 10\n",
    "    plot_every = 100\n",
    "    resume_from = \"\" \n",
    "\n",
    "args = Args()\n",
    "\n",
    "train(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "serbia-dish-image-generator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
