\documentclass[conference]{IEEEtran}

% --- PAKETI ---
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[serbian]{babel} % Srpski jezik

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{array}
\usepackage{hhline}
\usepackage{subcaption}
\usepackage{url}

% Linkovi
\definecolor{lightblue}{RGB}{0,191,255}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    urlcolor=lightblue,
    citecolor=black
}

% --- KOD BLOKOVI (ako zatrebaju) ---
\usepackage{listings}
\lstset{
  basicstyle=\ttfamily\footnotesize,
  keywordstyle=\color{blue}\bfseries,
  commentstyle=\color{gray}\itshape,
  stringstyle=\color{red},
  showstringspaces=false,
  frame=single,
  breaklines=true,
  language=Python
}

% --- POČETAK DOKUMENTA ---
\begin{document}

\title{Implementacija softverskog rešenja za generisanje slika srpskih tradicionalnih jela analizom recepata i sastojaka}

\author{%
\IEEEauthorblockN{Bojan Mijanović, Vukašin Bogdanović, Jovan Jokić}
\IEEEauthorblockA{\textit{Softversko inženjerstvo i informacione tehnologije}\\
\textit{Univerzitet u Novom Sadu, Fakultet tehničkih nauka}\\
Novi Sad, Srbija\\
\href{mailto:mijanovic.r23.2024@uns.ac.rs}{mijanovic.r23.2024@uns.ac.rs}\quad
\href{mailto:bogdanovic.r212.2024@uns.ac.rs}{bogdanovic.r212.2024@uns.ac.rs}\quad
\href{mailto:jokic.r219.2024@uns.ac.rs}{jokic.r219.2024@uns.ac.rs}}
}

\maketitle

% \begin{abstract}
% Ovde treba ubaciti kratak apstrakt rada.
% \end{abstract}

\section{Uvod}

U eri digitalne transformacije, veštačka inteligencija (AI) i mašinsko učenje postaju ključni alati u različitim sferama ljudskog delovanja, uključujući i očuvanje kulturnog nasleđa. Gastronomija, kao fundamentalni deo kulturnog identiteta jednog naroda, doživljava novu renesansu kroz digitalizaciju. Generativni modeli, posebno u domenu sinteze slike iz teksta (eng. \textit{text-to-image}), otvorili su revolucionarne mogućnosti za vizuelizaciju, edukaciju i promociju, omogućavajući kreiranje vizuelnog sadržaja iz prostog tekstualnog opisa.

Srpska tradicionalna kuhinja, sa svojom bogatom istorijom i raznovrsnošću, predstavlja značajan deo nacionalne baštine. Međutim, dok moderna kulinarska produkcija ima snažnu vizuelnu komponentu, ogroman deo tradicionalnog gastronomskog nasleđa, sačuvan u starim kuvarima, digitalizovanim arhivama i porodičnim beleškama, postoji isključivo u tekstualnoj formi. Ovi stariji recepti, prenošeni generacijama, često nemaju prateću sliku, što predstavlja ključnu prepreku za njihovu popularizaciju i očuvanje u digitalnom dobu. Bez vizuelne reprezentacije, korisnicima je teško da steknu pravu predstavu o krajnjem rezultatu jela, što smanjuje atraktivnost ovih recepata i otežava njihovo prenošenje novim generacijama koje su prevashodno vizuelno orijentisane.

Iako postoje moćni, opšte namenski generativni modeli trenirani na ogromnim skupovima podataka (npr. \textit{Stable Diffusion}, \textit{DALL-E}), oni često ne uspevaju da adekvatno generišu specifične, kulturološki nijansirane koncepte. Generisanje autentičnog prikaza lokalnih jela zahteva specifično poznavanje sastojaka i izgleda koji opšti modeli ne poseduju. Postoji jasna potreba za razvojem specijalizovanog rešenja koje može precizno interpretirati tekstualne opise (sastojke i korake pripreme) na srpskom jeziku i generisati fotorealistične prikaze koji odgovaraju tradicionalnoj srpskoj kuhinji.

Ovaj rad stoga predlaže razvoj i evaluaciju sistema koji transformiše tekstualni ulaz, specifično listu sastojaka i korake pripreme na srpskom jeziku u fotorealistični vizuelni izlaz koji predstavlja finalno jelo. Da bi se model obučio da razume semantičku vezu između teksta i slike u domenu srpske kuhinje, kreiraće se namenski skup podataka prikupljanjem recepata sa postojećih kulinarskih portala. Centralni deo istraživanja biće komparativna analiza različitih generativnih pristupa. Rad će uporediti performanse modela zasnovanih na arhitekturama kao što su kondicioni varijacioni autoenkoderi (cVAE) i kondicione generativne adversarijalne mreže (cGAN), treniranih na prikupljenom skupu podataka, sa rezultatima dobijenim finim podešavanjem (eng. \textit{fine-tuning}) velikih, prethodno obučenih modela (\textit{Stable Diffusion}) koristeći moderne i resursno efikasne tehnike poput LoRA (\textit{Low-Rank Adaptation}).

\section{Teorijske osnove}
Ovo poglavlje pruža teorijski pregled generativnih modela i tehnika koje se koriste u ovom radu.

\subsection{Varijacioni Autoenkoderi (VAE)}
Varijacioni autoenkoderi (VAE), koje su uveli Kingma i Welling \cite{ref1}, su generativni modeli koji pripadaju klasi modela zasnovanih na verovatnoći. Osnovna ideja VAE je da nauči latentnu reprezentaciju ($z$) ulaznih podataka ($x$). Sastoje se od dve glavne komponente: enkodera $q(z|x)$ i dekodera $p(x|z)$.

VAE se trenira maksimizacijom donje granice verovatnoće (eng. \textit{Evidence Lower Bound} - ELBO), što je ekvivalentno minimizaciji funkcije gubitka koja se sastoji od gubitka rekonstrukcije i Kullback-Leibler (KL) divergencije, koja deluje kao regularizator \cite{ref1}. Funkcija gubitka (ELBO) za VAE je:

\begin{equation} \label{eq:vae}
\mathcal{L}_{VAE} = \mathbb{E}_{q(z|x)}[\log p(x|z)] - D_{KL}(q(z|x) || p(z))
\end{equation}

Gde prvi član predstavlja očekivani logaritam verovatnoće rekonstrukcije, a drugi član je KL divergencija.

\subsection{Kondicioni Varijacioni Autoenkoderi (cVAE)}

Kondicioni varijacioni autoenkoderi (cVAE) su proširenje VAE modela koje omogućava kontrolisano generisanje \cite{ref2}. Ovo se postiže uslovljavanjem (eng. \textit{conditioning}) kako enkodera tako i dekodera dodatnim informacijama $c$ (npr. tekstualni embeding). Enkoder uči distribuciju $q(z|x, c)$, a dekoder $p(x|z, c)$. Funkcija gubitka se prilagođava da uključi ovaj uslov:

\begin{equation} \label{eq:cvae}
\mathcal{L}_{cVAE} = \mathbb{E}_{q(z|x, c)}[\log p(x|z, c)] - D_{KL}(q(z|x, c) || p(z|c))
\end{equation}

\subsection{Generativne Adversarijalne Mreže (GAN)}

Generativne adversarijalne mreže (GAN), koje je uveo Goodfellow et al. \cite{ref3}, predstavljaju pristup zasnovan na teoriji igara. GAN se sastoji od dve suprotstavljene neuronske mreže: Generatora ($G$) koji kreira podatke iz šuma $z$, i Diskriminatora ($D$) koji pokušava da razlikuje stvarne podatke ($x$) od lažnih ($G(z)$). Ove dve mreže igraju "minimax" igru definisanu ciljnom funkcijom:

\begin{equation} \label{eq:gan}
\begin{split}
\min_G \max_D V(D, G) = \mathbb{E}_{x \sim p_{data}(x)}[\log D(x)] \\ + \mathbb{E}_{z \sim p_z(z)}[\log(1 - D(G(z)))]
\end{split}
\end{equation}

\subsection{Kondicione Generativne Adversarijalne Mreže (cGAN)}

Mirza i Osindero \cite{ref4} su predložili kondicione generativne adversarijalne mreže (cGAN) kao proširenje koje omogućava kontrolu nad izlazom. I generatoru $G(z, c)$ i diskriminatoru $D(x, c)$ se prosleđuje dodatni uslov $c$. Ovo omogućava modelu da generiše slike koje odgovaraju specifičnom ulaznom tekstu, što je osnova za mnoge \textit{text-to-image} modele pre pojave difuzije.

\subsection{Difuzioni Modeli (Diffusion Models)}

Difuzioni modeli \cite{ref5}, \cite{ref6} su klasa generativnih modela koji postižu vrhunske rezultate (eng. \textit{state-of-the-art}) u sintezi slika. Rade na principu dva procesa: (1) proces unapred, gde se Gausov šum postepeno dodaje slici $x_0$ kroz $T$ koraka dok ne postane čisti šum $x_T$, i (2) proces unazad, gde se neuronska mreža (tipično U-Net) trenira da poništi ovaj proces, iterativno predviđajući i uklanjajući šum iz $x_T$ da bi se rekonstruisala $x_0$. Generisanje je uslovljeno (npr. tekstom) ubacivanjem vektora uslova $c$ u U-Net tokom procesa uklanjanja šuma.

\subsection{Stable Diffusion}

Standardni difuzioni modeli su računarski zahtevni jer rade u prostoru piksela. Rombach et al. \cite{ref7} su predstavili \textit{Latentne Difuzione Modele} (LDM), arhitekturu koja stoji iza \textit{Stable Diffusion} modela. LDM primenjuje difuzioni proces ne u prostoru piksela, već u \textit{latentnom prostoru} niže dimenzije. Slika se prvo kompresuje pomoću enkodera pre-treniranog VAE modela. Difuzija se vrši na ovoj latentnoj reprezentaciji, a zatim se rezultat dekodira nazad u prostor piksela pomoću VAE dekodera. Ovo drastično smanjuje računarsku složenost \cite{ref7}.

\subsection{LoRA (Low-Rank Adaptation)}

Fino podešavanje (\textit{fine-tuning}) modela sa milijardama parametara, kao što je \textit{Stable Diffusion}, je izuzetno skupo. LoRA (\textit{Low-Rank Adaptation}) \cite{ref8} je tehnika za efikasno fino podešavanje. LoRA zamrzava originalne težine modela ($W_0$) i ubacuje dve male, trenabilne matrice niskog ranga ($A$ i $B$) u ključne slojeve (npr. \textit{attention} slojeve). Tokom treninga, ažuriraju se samo parametri $A$ i $B$, gde je $\Delta W = BA$. Pošto je $r \ll d$ (gde je $r$ rang, a $d$ dimenzija), broj parametara za treniranje je drastično smanjen \cite{ref8}.

\subsection{FID (Fréchet Inception Distance)}

\textit{Fréchet Inception Distance} (FID) \cite{ref9} je standardna metrika za evaluaciju kvaliteta i raznovrsnosti generisanih slika. FID meri sličnost između distribucije stvarnih slika ($x$) i distribucije generisanih slika ($g$). Obe grupe slika se propuštaju kroz pre-treniranu Inception-v3 mrežu da bi se dobili embedinzi (aktivacije). Zatim se te dve distribucije embedinga modeliraju kao multivarijantne Gausove distribucije, izračunavanjem njihovih srednjih vrednosti ($\mu$) i kovarijansnih matrica ($\Sigma$). FID se izračunava kao Fréchet distanca između ove dve distribucije:

\begin{equation} \label{eq:fid}
FID(x, g) = ||\mu_x - \mu_g||^2 + \text{Tr}(\Sigma_x + \Sigma_g - 2(\Sigma_x \Sigma_g)^{1/2})
\end{equation}

Niža FID vrednost ukazuje na veću sličnost između distribucija stvarnih i generisanih slika \cite{ref9}.

\subsection{CLIPScore}

CLIPScore je metrika za procenu semantičke usklađenosti između generisane slike i pripadajućeg tekstualnog opisa. Zasniva se na CLIP (\textit{Contrastive Language–Image Pretraining}) modelu, koji zajednički uči reprezentacije teksta i slike mapirajući ih u isti latentni prostor \cite{ref10}.

Za datu sliku i tekstualni opis, CLIP enkoderi proizvode odgovarajuće vektorske reprezentacije koje se zatim upoređuju. CLIPScore se računa kao skalirana kosinusna sličnost između embeddinga slike i embeddinga teksta, pri čemu veće vrednosti ukazuju na jaču semantičku povezanost generisanog vizuelnog sadržaja i ulaznog opisa.

Za razliku od metrika koje mere samo vizuelni kvalitet ili statističku sličnost distribucija (npr. FID), CLIPScore direktno procenjuje koliko generisana slika odgovara značenju teksta, što ga čini posebno pogodnim za evaluaciju \textit{text-to-image} modela.

\subsection{CLIP Cosine Similarity}

CLIP cosine similarity predstavlja osnovnu meru sličnosti između tekstualnog i slikovnog embeddinga dobijenih iz CLIP modela. Izračunava se kao kosinus ugla između dva vektora u zajedničkom latentnom prostoru:

\begin{equation} \label{eq:clip_cosine}
\text{cos\_sim}(t, i) = \frac{t \cdot i}{\|t\|\|i\|}
\end{equation}

gde su $t$ i $i$ embedding vektori teksta i slike, respektivno. Vrednosti kosinusne sličnosti se nalaze u intervalu $[-1, 1]$, pri čemu veće vrednosti označavaju bolju semantičku usklađenost.

Ova metrika omogućava finiju analizu poravnanja teksta i slike i često se koristi zajedno sa CLIPScore-om kako bi se dobila stabilnija i interpretabilnija evaluacija generisanih rezultata.

\section{Srodni radovi}
Generisanje slika hrane na osnovu tekstualnog opisa predstavlja aktuelan izazov u domenu multimodalnog mašinskog učenja. Raniji pristupi, kao što je \textit{ChefGAN} koji su predložili Pan et al. \cite{pan2020chefgan}, uspešno su koristili kondicione generativne adversarijalne mreže (cGAN) za sintezu slika iz recepata, primenom kaskadnih modula za postizanje veće rezolucije i namenskih enkodera za tekst. Novija istraživanja se u velikoj meri oslanjaju na superiorne performanse difuzionih modela. Rad Ma et al. \cite{ma2024memory} predstavlja \textit{MLA-Diff} model, koji koristi difuziju poboljšanu memorijskim modulima i unapređenim CLIP enkoderom za rešavanje \textit{few-shot} problema generisanja na osnovu liste sastojaka. Pored samog generisanja, srodni radovi poput \textit{Foodfusion} (Shi et al. \cite{shi2024foodfusion}) bave se problemom kompozicije slika hrane, ali i, što je za ovaj rad posebno relevantno, rigoroznim metodologijama za kreiranje i preprocesiranje velikih skupova podataka, uključujući automatsku procenu i filtriranje slika lošeg kvaliteta. Ovaj rad se nadovezuje na ovu literaturu tako što direktno poredi klasične cGAN pristupe \cite{pan2020chefgan} sa modernim tehnikama finog podešavanja difuzionih modela (LoRA), koristeći CLIP kao centralni multimodalni enkoder po uzoru na \cite{ma2024memory}, ali na novom, specifičnom skupu podataka tradicionalne srpske kuhinje.

\section{Skup podataka}
Kvalitet i relevantnost skupa podataka su od presudnog značaja za uspešno treniranje generativnih modela, posebno u specifičnom domenu kao što je nacionalna kuhinja. S obzirom na to da ne postoji javno dostupan, anotiran skup podataka fokusiran na srpska tradicionalna jela pogodan za \textit{text-to-image} zadatke, kreiran je sopstveni skup podataka.

\subsection{Prikupljanje podataka}
Podaci su prikupljeni sa popularnih domaćih kulinarskih portala \textit{recepti.com} i \textit{coolinarica.com}, koji poseduju bogatu bazu recepata koje su postavljali korisnici. Za potrebe automatskog preuzimanja podataka, razvijene su namenske skripte za struganje podataka (eng. \textit{web scraping}) u programskom jeziku Python, koristeći biblioteke \textit{BeautifulSoup} i \textit{Selenium}.

Proces prikupljanja obuhvatio je preuzimanje slika jela, naziva recepata, liste sastojaka i tekstualnog opisa pripreme. Inicijalnim pokretanjem skripti prikupljen je sirovi skup podataka koji je brojao približno 30.000 unosa. Svi podaci su objedinjeni i strukturirani u jedinstveni JSON format, gde svaki objekat predstavlja instancu jela sa pratećim metapodacima.

\subsection{Filtriranje i čišćenje podataka}
Sirovi podaci prikupljeni sa interneta po prirodi sadrže veliku količinu šuma. Analizom inicijalnog skupa uočeni su sledeći problemi:
\begin{itemize}
    \item Slike izuzetno niske rezolucije ili lošeg osvetljenja.
    \item Slike koje ne prikazuju hranu (npr. slike ambalaže, pribora ili lica).
    \item Generičke slike preuzete sa interneta (stock fotografije) koje ne odgovaraju receptu.
    \item Duplikati istih jela pod različitim nazivima.
\end{itemize}

Zbog toga je sproveden proces ručnog čišćenja i verifikacije. Cilj je bio zadržati samo one slike koje su vizuelno jasne, estetski prihvatljive i koje verodostojno predstavljaju koncept tradicionalne kuhinje. Nakon eliminacije neadekvatnih primera, veličina skupa podataka je redukovana sa 30.000 na finalnih 7.000 visokokvalitetnih parova slika i teksta. Iako je ovo značajno smanjenje kvantiteta, kvalitet podataka je drastično povećan, što je ključno za stabilnost treninga i vernost generisanih rezultata.

\subsection{Preprocesiranje teksta i augmentacija}
Sirovi tekstualni podaci (sastojci i priprema) zahtevali su značajnu obradu pre upotrebe u modelima. Prvi korak podrazumevao je čišćenje teksta od HTML tagova, suvišnih razmaka i nestandardnih karaktera.

S obzirom na to da su osnovni modeli korišćeni u ovom radu (kao što je \textit{Stable Diffusion}) i njihovi tekstualni enkoderi (CLIP) primarno trenirani na engleskom jeziku, direktna upotreba srpskog jezika dovela bi do suboptimalnih rezultata. Takođe, originalni recepti su često predugački i sadrže narativne elemente nepotrebne za vizuelno generisanje.

Za rešavanje ovog problema korišćen je veliki jezički model GPT-4o (OpenAI). Implementiran je automatizovani \textit{pipeline} u kojem se modelu prosleđuje originalni naziv, sastojci i opis na srpskom jeziku, uz sistemsku instrukciju (eng. \textit{system prompt}) da izvrši dva zadatka:
\begin{enumerate}
    \item Prevod ključnih vizuelnih elemenata na engleski jezik.
    \item Sažimanje (eng. \textit{summarization}) teksta u koncizan opis (eng. \textit{caption}) koji je optimizovan za \textit{text-to-image} modele (fokus na boje, teksture, oblike i serviranje, a ne na proces kuvanja).
\end{enumerate}

\subsection{Generisanje embedinga}
Kao finalni korak pripreme podataka, tekstualni opisi dobijeni od GPT-4o modela konvertovani su u vektorski prostor (embedinge). Za ovu svrhu korišćen je CLIP (\textit{Contrastive Language-Image Pre-Training}) model \cite{radford2021learning}. CLIP projektuje tekst i sliku u zajednički latentni prostor, omogućavajući modelu da razume semantičku povezanost između vizuelnih i tekstualnih podataka. Ovi pre-komputirani embedinzi su korišćeni kao uslovni ulaz (kondicioniranje) tokom treniranja generativnih modela opisanih u sekciji Metodologija.

\section{Metodologija}
Ovo poglavlje opisuje tehničku implementaciju predloženog sistema. Proces se sastoji od tri ključne faze: (1) kodiranje tekstualnih opisa u vektorski prostor, (2) treniranje osnovnih generativnih modela (cVAE i cGAN) od nule na prikupljenom skupu podataka, i (3) fino podešavanje (eng. \textit{fine-tuning}) savremenog difuzionog modela korišćenjem tehnike LoRA.

\subsection{Pregled arhitekture sistema}
Ulaz u sistem predstavljaju preprocesirani tekstualni opisi jela na engleskom jeziku (dobijeni metodom opisanom u poglavlju 4). Ovi opisi se prvo propuštaju kroz pre-trenirani enkoder teksta kako bi se dobila bogata semantička reprezentacija (embeding). Dobijeni vektori služe kao uslov (kondicioniranje) za generativne modele koji na izlazu produku finalnu sliku jela.

\subsection{Kodiranje teksta i kondicioniranje}
Za razumevanje semantike teksta korišćen je \textit{CLIP (Contrastive Language-Image Pre-Training)} model \cite{radford2021learning}. Konkretno, korišćena je varijanta ViT-L/14 koja tekstualni opis preslikava u vektor fiksne dužine od 768 dimenzija.
Ovaj vektor se koristi kao uslovni ulaz $c$ u svim eksperimentima. S obzirom na to da je CLIP treniran na stotinama miliona parova slika-tekst, on omogućava modelima da razumeju koncepte (npr. "crvena boja", "tanjir", "čorba") mnogo bolje nego što bi to bilo moguće učenjem samo na našem ograničenom skupu podataka.

\subsection{Implementacija osnovnih modela}
U prvoj fazi eksperimenta, implementirani su i trenirani cVAE i cGAN modeli "od nule" (eng. \textit{from scratch}) na prikupljenom skupu podataka. Cilj je bio uspostaviti osnovnu liniju performansi (eng. \textit{baseline}). Slike su za potrebe ovih modela skalirane na rezoluciju od 128x128 piksela.

\subsubsection{Arhitektura cVAE}
Za implementaciju kondicionog varijacionog autoenkodera je korišćena konvuoluciona arhitektura zasnovana na CLIP embedinzima kao uslovnom signalu. Model se sastoji iz enkodera i dekodera, i adaptirani su za generisanje slika dimenzija 64x64. U daljem tekstu ćemo se osvrnuti na komponente \textit{cVAE} modela:

\paragraph{Enkoder} Arhitektura enkodera ima za cilj da transformiše ulazne slike u latentni prostor zadate dimenzionalnosti.\textit{CLIP} embedinzi se prostorno repliciraju i konkateniraju sa ulaznom slikom duž kanalne dimenzije. Enkoder se sastoji od četiri konvoluciona bloka. Nakon svakog konvolucionog bloka izuzev prvog, prisutna je \textit{batch} normalizacija i kao i deaktivacija određenog procenta neurona. Korišćena je ReLU aktivacija. Dva linearna sloja generišu parametre latentne distribucije (μ i log σ²). Uzorkovanje iz latentnog prostora vrši se pomoću reparametrizacionog trika: z = μ + σ · ε. 

\paragraph{Dekoder} Ulaz u dekoder predstavlja latentni vektor koji se konkatenira sa \textit{CLIP} embedingom. Dekoder se sastoji iz četiri konvoluciona transponovana bloka koji uvećavaju prostornu dimenzionalnost. Ovde se, kao i u enkoderu, koriste ReLU aktivacija i \textit{batch} normalizacija. Izuzetak predstavlja finalni sloj koji koristi Tanh aktivaciju. 
Trening je izvršen kroz 35 epoha, dok funkcija gubitka kombinuje \textit{MSE} (\textit{eng. Mean Squared Error}) i Kullback-Leibler divergenciju 
\subsubsection{Arhitektura cGAN}
Za implementaciju kondicionog GAN-a korišćena je arhitektura zasnovana na rezidualnim blokovima sa CLIP embedinzima kao uslovom.

\paragraph{Generator} sastoji se od niza ResNet blokova sa uzorkovanjem naviše (4×4 → 128×128). Ključna komponenta je Conditional Batch Normalization (CondBN) gde se parametri γ i β generišu iz tekstualnih embedinga. Koristi se ReLU aktivacija u skrivenim slojevima i Tanh na izlazu.

\paragraph{Diskriminator} je implementiran kao Projection Discriminator sa ResNet blokovima za smanjenje dimenzionalnosti. Umesto konkatenacije, koristi se tehnika projekcije: skalarni proizvod između vektora obeležja slike i projektovanog embedinga teksta.

\paragraph{Trening} koristi Hinge Loss, Adam optimizator ($\beta_1=0.0$, $\beta_2=0.9$), R1 gradient penalty, i EMA sa faktorom 0.999 za stabilnost.

\subsection{Fino podešavanje latentnog difuzionog modela}
Zbog visoke računske zahtevnosti potpunog treniranja difuzionih modela, u ovom radu je primenjen pristup efikasnog finog podešavanja (eng. \textit{Parameter-Efficient Fine-Tuning} - PEFT). Konkretno, korišćena je tehnika \textit{Low-Rank Adaptation} (LoRA) primenjena na pre-trenirani \textit{Stable Diffusion v1.5} model.

Umesto ažuriranja svih težina u U-Net mreži, LoRA zamrzava originalne parametre modela i uvodi parove dekompozicionih matrica niskog ranga u slojeve unakrsne pažnje (eng. \textit{cross-attention layers}). Ovo omogućava modelu da nauči specifične vizuelne karakteristike novog skupa podataka bez katastrofalnog zaboravljanja (eng. \textit{catastrophic forgetting}) prethodnog znanja, uz drastično manju potrošnju VRAM memorije.

Proces finog podešavanja fokusiran je na minimizaciju greške rekonstrukcije šuma, pri čemu model uči da generiše slike koje odgovaraju tekstualnim opisima iz domena našeg skupa podataka, zadržavajući pritom semantičko razumevanje stečeno na ogromnom skupu podataka (LAION-5B) na kojem je originalni model treniran.
\subsection{Eksperimentalno okruženje}
Svi modeli su trenirani i evaluirani na hardverskoj konfiguraciji koja uključuje NVIDIA GeForce RTX 5060 Ti grafičku karticu. Implementacija je urađena u programskom jeziku Python koristeći biblioteke \textit{PyTorch} i \textit{Diffusers}.

Hiperparametri treninga za cVAE model su bili sledeći:
\begin{itemize}
    \item Trajanje treninga: 35 epoha sa priodom čuvanja kontrolnih tačaka na svakih 5 epoha.
    \item Veličina paketa (eng. \textit{batch size}): 64. 
    \item Stopa učenja (eng. \textit{learning rate}):
     $2 \times 10^{-4}$, sa AdamW optimizatorom i weight decay faktorom od  $1 \times 10^{-5}$ za regularizaciju .
    \item Optimizator: Korišćen je AdamW optimizator sa parametrima $\beta_1=0.9$ i $\beta_2=0.999$.
    \item Regularizacija: Korišćeno je sečenje gradijenta (eng. \textit{gradient clipping}) sa normom $1.0$ kao i deaktivacija neurona sa stopom $1.0$ u enkoderu.
    \item Beta parametar: $\beta=0.5$ za ponderisanje \textit{KL} divergencije u funkciji gubitka.
    \item \textit{KL} zagrevanje: Primenjeno je linearno zagrevanje u periodu od 10 epoha za postepenu aktivaciju \textit{KL} divergencije u cilju sprečavanja kolapsa latentnog prostora u ranim fazama treninga.
    \item CLIP kondicioniranje: Korišćeni su  512-dimenzianalnih iz ViT-B/32 .
\end{itemize}

Hiperparametri treninga za cGAN model su bili sledeći:
\begin{itemize}
    \item Trajanje treninga: 40,000 iteracija.
    \item Veličina paketa (eng. \textit{batch size}): 4. Ova vrednost je odabrana zbog optimizacije za memorijska ograničenja RTX 5060 Ti GPU-a.
    \item Stopa učenja (eng. \textit{learning rate}):
    Za generator je korišćena konzervativna stopa od $5 \times 10^{-5}$, dok je za diskriminator korišćena nešto veća stopa od $1.5 \times 10^{-4}$ radi održavanja stabilnosti treninga.
    \item Optimizator: Korišćen je Adam optimizator sa parametrima $\beta_1=0.0$ i $\beta_2=0.9$.
    \item Regularizacija: Primenjen je R1 penal nad gradijentima ($\lambda=2.0$) radi stabilizacije diskriminatora, kao i sečenje gradijenta (eng. \textit{gradient clipping}) sa normom $1.0$.
    \item EMA (Exponential Moving Average): Korišćen je faktor opadanja od $0.999$ za težine generatora tokom inferencije, što je omogućilo stabilniju generaciju.
    \item Kapacitet modela: Osnovni broj kanala (eng. \textit{base channels}) je postavljen na 32 (umanjeno sa standardnih 64) kako bi se zadovoljila memorijska ograničenja hardvera.
    \item CLIP kondicioniranje: Korišćeni su 768-dimenzionalni embedinzi iz ViT-L/14 modela umesto standardnih 512-dimenzianalnih iz ViT-B/32 za bogatiju semantičku reprezentaciju.
\end{itemize}
Za fino podešavanje difuzionog modela korišćeni su sledeći parametri:
\begin{itemize}
    \item Osnovni model: Stable Diffusion v1.5.
    \item Trajanje treninga: 2,000 iteracija.
    \item Stopa učenja: Konstantna stopa od $1 \times 10^{-4}$.
    \item Rezolucija: Slike su redimenzionirane na $512 \times 512$ piksela.
    \item Optimizacija memorije: Korišćena je \textit{mixed-precision} (fp16) tehnika kako bi se smanjilo zauzeće memorije tokom treninga.
\end{itemize}

\section{Rezultati i diskusija}

\subsection{cVAE rezultati}

Proces treniranja cVAE modela kroz 35 epoha pokazuje uspešnu konvergenciju i stabilnost varijacionog učenja. Ukupan gubitak ({eng. total loss}) na početku treninga iznosi približno 2700 i rapidno opada tokom prvih 10 epoha, dostigavši vrednost oko 800. Nakon toga, model nastavlja da se postepeno poboljšava, sa finalnim vrednostima od približno 600 za trening skup i 680 za validacioni skup.
\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\columnwidth]{docs/images/cvae/losses_epoch_35.png}
\caption{Funkcija gubitka tokom treniranja cVAE kroz 35 epoha}
\label{fig:cvae_training}
\end{figure}
\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\columnwidth]{docs/images/cvae/generation_35.png}
\caption{Primeri slika srpskih jela nastalih korišćenjem cVAE modela.}
\label{fig:cgan_samples}
\end{figure}
\subsection{cGAN rezultati}

Proces treniranja cGAN modela kroz 40.000 iteracija pokazuje uspešnu adversarijalnu stabilizaciju. Generator loss se kreće u opsegu 0.5-2.0, dok diskriminator loss osciluje oko 1.0-1.5, što ukazuje na zdravu konkurenciju između komponenti. Model dostiže stabilnu konvergenciju oko 32.000 koraka bez znakova mode collapse-a ili training instabilnosti.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\columnwidth]{docs/images/cgan/training_losses_final.png}
\caption{Dinamika treninga cGAN modela kroz finalne korake (~32k iteracija). Prikazani su generator loss, discriminator loss i discriminator outputs (real/fake logits).}
\label{fig:cgan_training}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\columnwidth]{docs/images/cgan/generated_samples_grid.png}
\caption{Primeri generisanih slika srpskih jela i slika je korišćenjem cGAN modela. Grid prikazuje raznovrsnost generisanih jela uključujući sarmu, ćevape, gulaš, tradicionalne čorbe i pečena mesa.}
\label{fig:cgan_samples}
\end{figure}

\subsection{LoRA rezultati}

LoRA fine-tuning Stable Diffusion v1.5 modela evaluiran je kroz različite korake treninga sa dve konfiguracije: UNet-only i UNet + Text Encoder.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\columnwidth]{docs/images/sd_lora/training_loss.png}
\caption{Dinamika treninga LoRA modela. Prikazani su raw loss, EMA loss i rolling average kroz 500+ koraka treninga, demonstrirajući stabilnu konvergenciju.}
\label{fig:lora_training}
\end{figure}

\begin{table}[htbp]
\caption{Rezultati LoRA finog podešavanja}
\begin{center}
% \columnwidth osigurava da tabela ne prelazi sirinu kolone/teksta
\resizebox{\columnwidth}{!}{%
    \begin{tabular}{|l|c|c|c|c|}
    \hline
    \textbf{Konfiguracija} & \textbf{Korak} & \textbf{FID ↓} & \textbf{CLIPScore ↑} & \textbf{CLIP cos sim ↑} \\
    \hline
    UNet only & 200 & 165.43 & 64.52 & 0.2904 \\
    UNet only & 400 & 158.30 & 64.45 & 0.2890 \\
    \hline
    UNet + Text Encoder & 200 & \textbf{155.17} & \textbf{64.68} & \textbf{0.2936} \\
    UNet + Text Encoder & 400 & 164.53 & 64.42 & 0.2884 \\
    \hline
    \end{tabular}%
}
\label{tab:lora_results}
\end{center}
\end{table}

\newpage
UNet + Text Encoder konfiguracija postiže najbolji FID rezultat (155.17) na 200 koraka, dok UNet-only zahteva 400 koraka za optimalne performanse (158.30). Prošireni eksperiment sa 7K uzoraka postiže FID od 113.24, ali pokazuje znake overfitting-a u kvalitativnoj analizi.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\columnwidth]{docs/images/sd_lora/fid_steps.png}
\caption{Evolucija FID metrike kroz korake treninga za različite LoRA konfiguracije. UNet + Text Encoder konfiguracija postiže najbolje performanse na 200 koraka.}
\label{fig:lora_metrics}
\end{figure}

\begin{figure}[htbp]
\centering
\begin{subfigure}[b]{0.45\columnwidth}
    \includegraphics[width=\columnwidth]{docs/images/sd_lora/prompt_0_base.png}
    \caption{Bazni Stable Diffusion}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.45\columnwidth}
    \includegraphics[width=\columnwidth]{docs/images/sd_lora/prompt_0_lora.png}
    \caption{LoRA fine-tuned}
\end{subfigure}
\caption{Poređenje generisanja sarme: bazni model vs LoRA fine-tuned model. LoRA model pokazuje bolje razumevanje srpskog jela i tradicinalnog načina serviranja.}
\label{fig:lora_comparison}
\end{figure}

\subsection{Komparativna analiza}

Poređenje cVAE, cGAN i LoRA pristupa otkriva komplementarne prednosti:

\paragraph{cVAE prednosti} Stabilniji trening bez adversarijalne dinamike i problema mode collapse-a karakterističnih za cGAN i brža inferencija sa jednim forward pass-om umesto iterativnog denoising procesa 

\paragraph{cGAN prednosti} Potpuna kontrola nad arhitekturom, kompaktniji model za deployment, trening isključivo na ciljanom skupu podataka.

\paragraph{LoRA prednosti} Superiorna vizuelna kvaliteta zahvaljujući pretreniranim težinama, viša rezolucija (512×512 vs 128×128), brža konvergencija.

\paragraph{Kvantitativno poređenje:} LoRA postiže niže FID vrednosti (~155 vs cGAN i cVAE koji nema numeričku FID procenu), dok sva tri pristupa postižu slične CLIPScore nivoe (~64-65). LoRA omogućava fine-tuning velikih modela sa dramatično smanjenim resursima, dok cGAN zahteva precizniju regulaciju hiperparametara za stabilnost.

\subsection{Diskusija i ograničenja}

\subsubsection{Inherentna ograničenja cGAN pristupa}

Rezultati jasno pokazuju da cGAN i cVAE modeli trenirani od nule ne mogu da se mere sa performansama LoRA fine-tuned Stable Diffusion modela. Ova razlika nije iznenađujuća i proističe iz nekoliko fundamentalnih ograničenja:

\paragraph{Ograničeni kapacitet skupa podataka:} Sa približno 7.000 slika, skup podataka je značajno manji od milijardi primera na kojima su trenirani modeli kao što je Stable Diffusion. cGAN i cVAE modeli moraju da uče reprezentacije "od nule" bez prethodnog znanja o vizuelnom svetu, što rezultuje:

\begin{itemize}
    \item Čestim geometrijskim deformacijama u generisanim slikama
    \item Nekonzistentnim teksturama i površinama
    \item Ograničenim razumevanjem prostornih odnosa između objekata
    \item Teškoćama u generisanju finih detalja karakterističnih za srpska jela
\end{itemize}

\paragraph{Arhitekturna ograničenja:} cGAN model sa 128×128 rezolucijom i smanjenim brojem kanala (32), kao i cVAE sa identičnom ili čak nižom rezolucijom predstavlja značajan kompromis u odnosu na 512×512 LoRA model. Ova ograničenja su bila neophodna zbog memorijskih ograničenja GPU-a, ali direktno utiču na kvalitet generisanih slika.

\paragraph{Potreba za proširenim treniranjem:} Trenutni rezultati ukazuju da bi cGAN model trebalo značajno duži trening period i veći skup podataka za postizanje zadovoljavajućeg kvaliteta. Procenjuje se da bi bilo potrebno:

\begin{itemize}
    \item Najmanje 20.000-30.000 visokokvalitetnih slika srpskih jela
    \item 100.000-200.000 iteracija treninga za postizanje stabilnosti
    \item Viša rezolucija (256×256 ili 512×512) za bolju reprezentaciju detalja
    \item Naprednije regularizacijske tehnike za sprečavanje overfitting-a
\end{itemize}

\subsubsection{Prednosti LoRA pristupa u praktičnim scenarijima}

LoRA fine-tuning se pokazao kao superioran pristup za praktične primene iz nekoliko razloga:

\begin{itemize}
    \item Leveraging pretrained knowledge: Korišćenje već naučenih reprezentacija sa LAION-5B skupa podataka omogućava modelu da se fokusira na specifičnosti srpske kuhinje umesto na fundamentalno učenje vizuelnih koncepta
    \item Resource efficiency: Značajno manji računski troškovi u odnosu na trening cGAN-a od nule
    \item Quality guarantee: Predvidljiv kvalitet rezultata zahvaljujući stabilnoj osnovi Stable Diffusion modela
\end{itemize}

\newpage
\section{Zaključak}

Ovaj rad je predstavio sveobuhvatnu studiju o primeni generativnih modela veštačke inteligencije u cilju digitalnog očuvanja i promocije srpskog kulinarskog nasleđa. Kroz razvoj namenskog skupa podataka i komparativnu analizu tri različite arhitekture, demonstrirano je da primena naprednih metoda dubokog učenja može uspešno premostiti jaz između tekstualnih zapisa i vizuelne reprezentacije. Kreiranje i kuriranje prvog multimodalnog skupa podataka srpske tradicionalne kuhinje, koji se sastoji od 7.000 parova slika i teksta, predstavlja temeljni doprinos ovog istraživanja. Pokazalo se da je kvalitet podataka, uz semantičko obogaćivanje pomoću CLIP i GPT modela, presudan faktor za uspešno treniranje, koji u specifičnim domenima nadmašuje značaj samog kvantiteta sirovih podataka.

Rezultati komparativne analize jasno ukazuju na superiornost adaptivnog pristupa. Fino podešavanje \textit{Stable Diffusion} modela tehnikom LoRA (FID 155.17) omogućilo je generisanje fotorealističnih prikaza visoke rezolucije, efikasno koristeći prethodno stečeno "opšte znanje" modela i prilagođavajući ga specifičnostima srpske gastronomije. Nasuprot tome, modeli trenirani od nule, poput cVAE i cGAN, iako sposobni za učenje osnovnih distribucija boja i oblika, pokazali su ograničenja u vidu niže rezolucije i pojave geometrijskih artefakata. Analiza sugeriše da bi za postizanje uporedivog kvaliteta ovim pristupima bio neophodan drastično veći skup podataka i značajniji računarski resursi, što ih čini manje praktičnim za domene sa ograničenim izvorima podataka.

Zaključno, istraživanje potvrđuje da je za digitalizaciju i vizuelizaciju kulturnog nasleđa strategija prenosa znanja (eng. \textit{transfer learning}) najefikasniji put. LoRA nudi optimalan balans između kvaliteta i resursa, čineći ovakva rešenja održivim i dostupnim. Buduća istraživanja će se usmeriti ka daljem proširenju skupa podataka i istraživanju hibridnih arhitektura koje bi mogle dodatno unaprediti vernost i brzinu generisanja, otvarajući nove mogućnosti za interaktivnu edukaciju i promociju nacionalne baštine.

% Bibliografija
\newpage
\bibliographystyle{IEEEtran}
\bibliography{bibliography}

\end{document}
