{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# CVAE\n",
    "Dataset"
   ],
   "id": "2a5b6a7cfdf0f131"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os, glob, numpy as np, torch\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from torch.nn import Module, Sequential, Conv2d, Dropout2d, BatchNorm2d, Flatten, Linear, ConvTranspose2d, ReLU, Tanh, \\\n",
    "    LeakyReLU, init\n",
    "import argparse\n",
    "import time\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "from torch import device, cuda, save, load, no_grad, cat, clamp, randn\n",
    "from torch.nn.functional import mse_loss\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.optim import AdamW\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dataset import CaptionImageSet\n",
    "from model import ConvCVAE\n",
    "\n",
    "SUPPORTED_IMG_EXTENSIONS = (\".jpg\", \".jpeg\", \".png\", \".webp\")\n",
    "np.random.seed(2)\n",
    "torch.manual_seed(2)\n",
    "\n",
    "device = device(\"cuda\" if cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "1f1fd48ae612b0d9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CaptionImageSet(Dataset):\n",
    "    \"\"\"Dataset class created to load images and its embeddings\"\"\"\n",
    "\n",
    "    def __init__(self, root='data/processed', img_size=64, test_split=0.1, is_test=False):\n",
    "        super(CaptionImageSet,self).__init__()\n",
    "        self.img_dir = os.path.join(root, \"images\")\n",
    "        self.emb_dir = os.path.join(root, \"embeds\")\n",
    "        self.img_size = img_size\n",
    "        self.ids = []\n",
    "        self.test_split = test_split\n",
    "        self.is_test = is_test\n",
    "        if is_test:\n",
    "            self.preprocess = transforms.Compose([\n",
    "                transforms.Resize((self.img_size, self.img_size)),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
    "        else:\n",
    "            self.preprocess = transforms.Compose([\n",
    "                transforms.Resize((img_size, img_size)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "            ])\n",
    "\n",
    "        if not os.path.exists(self.img_dir):\n",
    "            raise ValueError(f\"Image directory not found: {self.img_dir}\")\n",
    "        if not os.path.exists(self.emb_dir):\n",
    "            raise ValueError(f\"Embedding directory not found: {self.emb_dir}\")\n",
    "\n",
    "        self._load_ids()\n",
    "        self._split_data()\n",
    "\n",
    "        print(f\"Loaded {'test' if is_test else 'train'} dataset: {len(self.ids)} samples\")\n",
    "\n",
    "    def _load_ids(self):\n",
    "        for file_name in glob.glob(os.path.join(self.img_dir, \"*\")):\n",
    "            base_name, extension_name = os.path.splitext(os.path.basename(file_name))\n",
    "            if extension_name.lower() in SUPPORTED_IMG_EXTENSIONS and os.path.exists(\n",
    "                    os.path.join(self.emb_dir, base_name + \".npy\")):\n",
    "                self.ids.append(base_name)\n",
    "        self.ids.sort()\n",
    "        print(f\"Loaded {len(self.ids)} valid pairs of images and embeddings\")\n",
    "\n",
    "    def _split_data(self):\n",
    "        if self.test_split > 0:\n",
    "            n_test = int(len(self.ids) * self.test_split)\n",
    "            if self.is_test:\n",
    "                self.ids = self.ids[:n_test]\n",
    "            else:\n",
    "                self.ids = self.ids[n_test:]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        id = self.ids[idx]\n",
    "\n",
    "        img_path = None\n",
    "        img_base_path = os.path.join(self.img_dir, id)\n",
    "        for ext in SUPPORTED_IMG_EXTENSIONS:\n",
    "            if os.path.exists(img_base_path + ext):\n",
    "                img_path = img_base_path + ext\n",
    "                break\n",
    "\n",
    "        if img_path is None:\n",
    "            raise ValueError(f\"Image path {id} not found\")\n",
    "\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        embedding = np.load(os.path.join(self.emb_dir, id + \".npy\")).astype(np.float32)\n",
    "        if embedding.ndim > 1:\n",
    "            embedding = embedding.flatten()\n",
    "\n",
    "        return self.preprocess(img), torch.from_numpy(embedding)\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model\n",
   "id": "b952bc1ef1c7b7c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class Encoder(Module):\n",
    "    '''Encoder component'''\n",
    "    def __init__(self, latent_dim, img_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.img_dim = img_dim\n",
    "        self.encoder_input = self.img_dim[-1] + 512\n",
    "        self.layers = Sequential(\n",
    "            Conv2d(self.encoder_input, 64, kernel_size=4, stride=2, padding=1),\n",
    "            LeakyReLU(0.2),\n",
    "            Dropout2d(0.1),\n",
    "\n",
    "            Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            BatchNorm2d(128),\n",
    "            LeakyReLU(0.2),\n",
    "            Dropout2d(0.1),\n",
    "\n",
    "            Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            BatchNorm2d(256),\n",
    "            LeakyReLU(0.2),\n",
    "            Dropout2d(0.1),\n",
    "\n",
    "            Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            BatchNorm2d(512),\n",
    "            LeakyReLU(0.2),\n",
    "            Flatten(),\n",
    "        )\n",
    "\n",
    "        self.mean = Linear(512 * 4 * 4, latent_dim)\n",
    "        self.log_of_variance = Linear(512 * 4 * 4, latent_dim)\n",
    "\n",
    "\n",
    "    def forward(self, img, embedding):\n",
    "        embedding = embedding.view(embedding.size(0), 512, 1, 1).repeat(1, 1, self.img_dim[0], self.img_dim[1])\n",
    "        result = cat((img, embedding), dim=1)\n",
    "        x = self.layers(result)\n",
    "        mean = self.mean(x)\n",
    "        log_of_var = clamp(self.log_of_variance(x), min=-10, max=10)\n",
    "        return mean, log_of_var"
   ],
   "id": "e4c686f4dc720516"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class Decoder(Module):\n",
    "    \"\"\"Decoder component\"\"\"\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decoder_input = Linear(latent_dim + 512, 512 * 4 * 4)\n",
    "        self.layers = Sequential(\n",
    "            ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n",
    "            BatchNorm2d(256),\n",
    "            ReLU(),\n",
    "\n",
    "            ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            BatchNorm2d(128),\n",
    "            ReLU(),\n",
    "\n",
    "            ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            BatchNorm2d(64),\n",
    "            ReLU(),\n",
    "\n",
    "            ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1),\n",
    "            Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z, clip_embedding):\n",
    "        z = cat([z, clip_embedding], dim=1)\n",
    "        x = self.decoder_input(z)\n",
    "        x = x.view(-1, 512, 4, 4)\n",
    "        x = self.layers(x)\n",
    "        return x\n"
   ],
   "id": "702cb6bfa133756f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class ConvCVAE(Module):\n",
    "    \"\"\"Conditional Variational Autoencoder.\"\"\"\n",
    "\n",
    "    def __init__(self, img_dim=[64, 64, 3], latent_dim=128):\n",
    "        super(ConvCVAE, self).__init__()\n",
    "        self.img_size = img_dim[0]\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.encoder = Encoder(img_dim=img_dim,latent_dim=latent_dim)\n",
    "\n",
    "        self.decoder = Decoder(latent_dim=latent_dim)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, (Conv2d, ConvTranspose2d)):\n",
    "            init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='leaky_relu')\n",
    "            if m.bias is not None:\n",
    "                init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, BatchNorm2d):\n",
    "            init.constant_(m.weight, 1)\n",
    "            init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, Linear):\n",
    "            init.xavier_normal_(m.weight)\n",
    "            init.constant_(m.bias, 0)\n",
    "\n",
    "    def reparameterize(self, mean, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        e = torch.randn_like(std)\n",
    "        z = mean + e * std\n",
    "        return z\n",
    "\n",
    "    def encode(self, x, embedding):\n",
    "        return self.encoder(x, embedding)\n",
    "\n",
    "    def decode(self, z, embedding):\n",
    "        return self.decoder(z, embedding)\n",
    "\n",
    "    def forward(self, x, embedding):\n",
    "        mean, log_var = self.encode(x, embedding)\n",
    "        z = self.reparameterize(mean, log_var)\n",
    "        recon_x = self.decode(z, embedding)\n",
    "\n",
    "        return recon_x, mean, log_var\n",
    "\n",
    "    def generate(self, embedding, num_samples=1):\n",
    "        embedding = embedding.unsqueeze(0).repeat(num_samples, 1) if len(embedding.shape) == 1 else embedding\n",
    "\n",
    "        z = torch.randn(embedding.size(0), self.latent_dim).to(embedding.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            generated = self.decode(z, embedding)\n",
    "\n",
    "        return generated\n",
    "\n",
    "    def reconstruct(self, x, clip_embedding):\n",
    "        with torch.no_grad():\n",
    "            mean, _ = self.encode(x, clip_embedding)\n",
    "            recon = self.decode(mean, clip_embedding)\n",
    "        return recon\n"
   ],
   "id": "a8d3f92a340a14a0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "def load_dataset(args):\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"LOADING DATASET\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    train_dataset = CaptionImageSet(\n",
    "        root=args.data_root,\n",
    "        img_size=args.img_size,\n",
    "        test_split=args.test_split,\n",
    "        is_test=False\n",
    "    )\n",
    "\n",
    "    test_dataset = CaptionImageSet(\n",
    "        root=args.data_root,\n",
    "        img_size=args.img_size,\n",
    "        test_split=args.test_split,\n",
    "        is_test=True\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def generate_img_grid(images, save_path, title):\n",
    "    images = clamp((images * 0.5 + 0.5), 0, 1)\n",
    "    grid = make_grid(images, nrow=4)\n",
    "    np_grid = grid.cpu().numpy()\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(np.transpose(np_grid, (1, 2, 0)))\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def loss_function(recon_x, x, mean, log_var, beta=1.0, warmup_factor=1.0):\n",
    "    recon_loss = mse_loss(recon_x, x, reduction='sum')\n",
    "    kl_loss = clamp(-0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp()), max=1e6)\n",
    "    total_loss = recon_loss + (beta * warmup_factor * kl_loss)\n",
    "\n",
    "    return total_loss, recon_loss, kl_loss\n",
    "\n",
    "\n",
    "def get_warmup_factor(epoch, warmup_epochs=10):\n",
    "    if epoch < warmup_epochs:\n",
    "        return epoch / warmup_epochs\n",
    "    return 1.0\n",
    "\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer, beta, epoch, warmup_epochs):\n",
    "    model.train()\n",
    "    tr_loss, tr_recon_loss, tr_kl = 0, 0, 0\n",
    "\n",
    "    warmup_factor = get_warmup_factor(epoch, warmup_epochs)\n",
    "\n",
    "    for batch_idx, (data, clip_emb) in enumerate(tqdm(train_loader)):\n",
    "        data = data.to(device)\n",
    "        clip_emb = clip_emb.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mean, log_var = model(data, clip_emb)\n",
    "        loss, recon_loss, kl_loss = loss_function(recon_batch, data, mean, log_var, beta, warmup_factor)\n",
    "\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        tr_recon_loss += recon_loss.item()\n",
    "        tr_kl += kl_loss.item()\n",
    "\n",
    "    n_samples = len(train_loader.dataset)\n",
    "    return tr_loss / n_samples, tr_recon_loss / n_samples, tr_kl / n_samples, warmup_factor\n",
    "\n",
    "\n",
    "def validate(model, val_loader, beta):\n",
    "    model.eval()\n",
    "    val_loss, val_recon, val_kl = 0, 0, 0\n",
    "\n",
    "    with no_grad():\n",
    "        for data, clip_emb in val_loader:\n",
    "            data = data.to(device)\n",
    "            clip_emb = clip_emb.to(device)\n",
    "            recon_batch, mean, log_var = model(data, clip_emb)\n",
    "            loss, recon_loss, kl_loss = loss_function(recon_batch, data, mean, log_var, beta, warmup_factor=1.0)\n",
    "            val_loss += loss.item()\n",
    "            val_recon += recon_loss.item()\n",
    "            val_kl += kl_loss.item()\n",
    "\n",
    "    n_samples = len(val_loader.dataset)\n",
    "    return val_loss / n_samples, val_recon / n_samples, val_kl / n_samples\n",
    "\n",
    "\n",
    "def train(args):\n",
    "\n",
    "    def save_plots_images(label):\n",
    "        data_iter = iter(test_loader)\n",
    "        images, embeddings = next(data_iter)\n",
    "\n",
    "        model.eval()\n",
    "        with no_grad():\n",
    "            images = images.to(device)\n",
    "            embeddings = embeddings.to(device)\n",
    "            recon, _, _ = model(images, embeddings)\n",
    "\n",
    "        comparison = cat([images[:8].cpu(), recon[:8].cpu()])\n",
    "        save_path = os.path.join(result_dir, f\"reconstruction_epoch_{label}.png\")\n",
    "        generate_img_grid(comparison, save_path, f\"Reconstruction - Epoch {label}\")\n",
    "\n",
    "        generated = model.generate(embeddings[:16], num_samples=16)\n",
    "        save_path = os.path.join(result_dir, f\"generated_epoch_{label}.png\")\n",
    "        generate_img_grid(generated, save_path, f\"Generated - Epoch {label}\")\n",
    "\n",
    "        plot_losses(history, os.path.join(loss_dir, f\"losses_epoch_{label}.png\"))\n",
    "\n",
    "    start_time_total = time.perf_counter()\n",
    "    dt = datetime.now()\n",
    "    dt_str = dt.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "    output_root = f'{dt_str}_{args.output_dir}'\n",
    "    os.makedirs(output_root, exist_ok=True)\n",
    "\n",
    "    checkpoint_dir = os.path.join(output_root, \"checkpoints\")\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    result_dir = os.path.join(output_root, \"results\")\n",
    "    os.makedirs(result_dir, exist_ok=True)\n",
    "\n",
    "    loss_dir = os.path.join(output_root, \"losses\")\n",
    "    os.makedirs(loss_dir, exist_ok=True)\n",
    "\n",
    "    train_loader, test_loader = load_dataset(args)\n",
    "\n",
    "    model = ConvCVAE(img_dim=[args.img_size, args.img_size, 3],latent_dim=args.latent_dim).to(device)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=args.lr, weight_decay=1e-5)\n",
    "\n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "    print(f\"\\nModel Architecture:\")\n",
    "    print(f\"  Latent dim: {args.latent_dim}\")\n",
    "    print(f\"  CLIP dim: 512\")\n",
    "    print(f\"  Image size: {args.img_size}\")\n",
    "    print(f\"  Beta: {args.beta}\")\n",
    "    print(f\"  KL warmup epochs: {args.warmup_epochs}\")\n",
    "\n",
    "    epoch_idx = 0\n",
    "    min_loss = float('inf')\n",
    "\n",
    "    if args.checkpoint:\n",
    "        checkpoint = load(args.checkpoint, map_location=device)\n",
    "        epoch_idx = checkpoint['epoch_idx'] + 1\n",
    "        model.load_state_dict(checkpoint['model_state'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
    "        min_loss = checkpoint.get('min_loss', float('inf'))\n",
    "        print(f\"\\nLoaded checkpoint from epoch {checkpoint['epoch']}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"STARTING TRAINING\")\n",
    "    print(\"=\" * 70)\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_recon': [],\n",
    "        'train_kl': [],\n",
    "        'val_loss': [],\n",
    "        'val_recon': [],\n",
    "        'val_kl': []\n",
    "    }\n",
    "\n",
    "    for epoch in range(epoch_idx, args.epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{args.epochs}\")\n",
    "\n",
    "        train_loss, train_recon, train_kl, warmup = train_epoch(\n",
    "            model, train_loader, optimizer, args.beta, epoch, args.warmup_epochs\n",
    "        )\n",
    "\n",
    "        val_loss, val_recon, val_kl = validate(model, test_loader, args.beta)\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_recon'].append(train_recon)\n",
    "        history['train_kl'].append(train_kl)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_recon'].append(val_recon)\n",
    "        history['val_kl'].append(val_kl)\n",
    "\n",
    "        print(f\"Warmup: {warmup:.2f} | LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        print(f\"Train - Loss: {train_loss:.4f}, Recon: {train_recon:.4f}, KL: {train_kl:.4f}\")\n",
    "        print(f\"Val   - Loss: {val_loss:.4f}, Recon: {val_recon:.4f}, KL: {val_kl:.4f}\")\n",
    "\n",
    "        # Save best model\n",
    "        if val_loss < min_loss:\n",
    "            min_loss = val_loss\n",
    "            save({\n",
    "                'epoch_idx': epoch,\n",
    "                'model_state': model.state_dict(),\n",
    "                'optimizer_state': optimizer.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'min_loss': min_loss,\n",
    "            }, os.path.join(checkpoint_dir, \"best_checkpoint.pt\"))\n",
    "            print ('Best checkpoint saved')\n",
    "            save_plots_images('best')\n",
    "\n",
    "\n",
    "        if (epoch + 1) % args.save_interval == 0:\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f\"model_epoch_{epoch + 1}.pt\")\n",
    "            save({\n",
    "                'epoch_idx': epoch,\n",
    "                'model_state': model.state_dict(),\n",
    "                'optimizer_state': optimizer.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'min_loss': min_loss,\n",
    "            }, checkpoint_path)\n",
    "            print(f\"Checkpoint saved: {checkpoint_path}\")\n",
    "\n",
    "            save_plots_images(str(epoch + 1))\n",
    "\n",
    "    final_path = os.path.join(checkpoint_dir, \"model_final.pt\")\n",
    "    save({\n",
    "        'epoch_idx': args.epochs - 1,\n",
    "        'model_state': model.state_dict(),\n",
    "        'optimizer_state': optimizer.state_dict(),\n",
    "        'min_loss': min_loss,\n",
    "    }, final_path)\n",
    "\n",
    "    plot_losses(history, os.path.join(loss_dir, \"losses_final.png\"))\n",
    "\n",
    "    total_time = time.perf_counter() - start_time_total\n",
    "    print(f\"\\n\" + \"=\" * 70)\n",
    "    print(f\"TRAINING COMPLETE\")\n",
    "    print(f\"Best validation loss: {min_loss:.4f}\")\n",
    "    print(f\"Total time: {total_time:.2f}s ({total_time / 60:.2f} minutes)\")\n",
    "    print(f\"Results saved to: {output_root}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_losses(history, save_path):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "    axes[0].plot(epochs, history['train_loss'], 'b-', label='Train')\n",
    "    axes[0].plot(epochs, history['val_loss'], 'r-', label='Val')\n",
    "    axes[0].set_title('Total Loss')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "\n",
    "    axes[1].plot(epochs, history['train_recon'], 'b-', label='Train')\n",
    "    axes[1].plot(epochs, history['val_recon'], 'r-', label='Val')\n",
    "    axes[1].set_title('Reconstruction Loss')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Loss')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True)\n",
    "\n",
    "    axes[2].plot(epochs, history['train_kl'], 'b-', label='Train')\n",
    "    axes[2].plot(epochs, history['val_kl'], 'r-', label='Val')\n",
    "    axes[2].set_title('KL Divergence')\n",
    "    axes[2].set_xlabel('Epoch')\n",
    "    axes[2].set_ylabel('Loss')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, bbox_inches='tight', dpi=150)\n",
    "    plt.close()\n"
   ],
   "id": "e6a9db3d4eebce66"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "parser = argparse.ArgumentParser(description=\"Train CLIP-CVAE on food images\")\n",
    "parser.add_argument(\"--data_root\", type=str, default=\"data/processed\",\n",
    "                    help=\"Root directory containing images and embeds folders\")\n",
    "parser.add_argument(\"--output_dir\", type=str, default=\"results_pytorch\",\n",
    "                    help=\"Output directory for checkpoints and samples\")\n",
    "parser.add_argument(\"--img_size\", type=int, default=64,\n",
    "                    help=\"Image size (height and width)\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=32,\n",
    "                    help=\"Batch size for training\")\n",
    "parser.add_argument(\"--test_split\", type=float, default=0.1,\n",
    "                    help=\"Fraction of data to use for testing\")\n",
    "parser.add_argument(\"--latent_dim\", type=int, default=128,\n",
    "                    help=\"Latent dimension size\")\n",
    "parser.add_argument(\"--beta\", type=float, default=0.5,\n",
    "                    help=\"Beta parameter for KL loss weighting\")\n",
    "parser.add_argument(\"--lr\", type=float, default=2e-4,\n",
    "                    help=\"Learning rate\")\n",
    "parser.add_argument(\"--epochs\", type=int, default=100,\n",
    "                    help=\"Number of training epochs\")\n",
    "parser.add_argument(\"--warmup_epochs\", type=int, default=10,\n",
    "                    help=\"Number of epochs for KL warmup\")\n",
    "parser.add_argument(\"--save_interval\", type=int, default=5,\n",
    "                    help=\"Save checkpoint every N epochs\")\n",
    "parser.add_argument(\"--checkpoint\", type=str, default=None,\n",
    "                    help=\"Path to checkpoint to resume training\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "train(args)"
   ],
   "id": "f66d1ed7c925cd0b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
